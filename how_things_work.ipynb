{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Project\n",
    "\n",
    "A rover has landed on Mars.\n",
    "\n",
    "It knows its landing point and final destination but can only perceive its immediate surroundings. The objective is to reach the target while minimizing path length and energy consumption.\n",
    "\n",
    "Environment:\n",
    "- The terrain is modeled from a Mars DTM and simplified into a grid world, where each pixel corresponds to 1m. The rover, sized 1m×1m, occupies a single cell.\n",
    "- The rover’s perception is limited to its local field of view. Using the DTM, visibility is computed in a 360° radius, restricted by terrain occlusions (e.g., it cannot see behind hills).\n",
    "\n",
    "The task is approached using reinforcement learning with different techniques."
   ],
   "id": "46f8100a98714411"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:34:56.728730Z",
     "start_time": "2025-09-30T09:34:53.874301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import device\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from custom_environment import GridMarsEnv\n",
    "from hirise_dtm import HiriseDTM\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ],
   "id": "a9d48dd4f6b0e58d",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Digital Terrain Models (DTMs)\n",
    "\n",
    "The HiRISE DTM is a 32-bit .IMG format. The .IMG is a PDS standard raster file format.\n",
    "\n",
    "Data is taken from the [HiRISE Dataset](https://www.uahirise.org/dtm/).\n",
    "\n",
    "HiRISE DTMs are created from two images of the same area, captured from different viewing angles. Their main advantage lies in the high resolution of the source images. The final product typically achieves a horizontal resolution of about 1–2 m/px and a vertical precision on the order of tens of centimeters. More info about DTM production can be found [here](https://www.uahirise.org/dtm/about.php).\n"
   ],
   "id": "e1113b128a5d3043"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-30T09:34:58.293734Z",
     "start_time": "2025-09-30T09:34:56.730822Z"
    }
   },
   "source": [
    "#filepath = \"DTMs/DTEEC_016460_2230_016170_2230_G01.IMG\"\n",
    "filepath = \"DTMs/DTEED_082989_1630_083055_1630_A01.IMG\"\n",
    "dtm_file = HiriseDTM(filepath)\n",
    "\n",
    "dtm_file.plot_dtm()\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:34:58.678998Z",
     "start_time": "2025-09-30T09:34:58.295545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# portion of the map extraction\n",
    "subset, position = dtm_file.get_portion_of_map(size=100)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(subset, cmap=\"terrain\")\n",
    "plt.colorbar(label=\"Elevation (m)\")\n",
    "plt.title(f\"HiRISE DTM Subset, from row:{position[1]}, column:{position[0]}\")\n",
    "plt.show()"
   ],
   "id": "ed691abe844331f1",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:34:58.833514Z",
     "start_time": "2025-09-30T09:34:58.680009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the file metadata gives you information about the file. grid_spacing is the precision of the map. grid_spacing=1 means that 1px=1m^2\n",
    "dtm_file.metadata\n"
   ],
   "id": "dc70384ebe8ba2a",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gymnasium Library\n",
    "\n",
    "The [Gymnasium library](https://gymnasium.farama.org/index.html) is an open-source toolkit for reinforcement learning that provides a standard interface and a wide range of environments to develop, test, and benchmark RL algorithms. It allows to create your own custom environment.\n",
    "\n",
    "[Here](https://gymnasium.farama.org/introduction/basic_usage/) you can find a small introduction to the library."
   ],
   "id": "15002f1df39ed01d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:34:58.989167Z",
     "start_time": "2025-09-30T09:34:58.834521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "grid_mars_env = GridMarsEnv(dtm_file, render_mode=\"ascii\", map_size=5, fov_distance=1, rover_max_drop=0.6, rover_max_step=0.6)\n",
    "obs, info = grid_mars_env.reset(seed=11)\n"
   ],
   "id": "380949c7ecb314b0",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:34:59.129604Z",
     "start_time": "2025-09-30T09:34:58.992164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = grid_mars_env.find_best_path(use_slope_cost=False)\n",
    "if path is not None:\n",
    "    print(f\"Path of length {len(path)} found\")\n",
    "    grid_mars_env.render_ascii(path)\n",
    "else:\n",
    "    print(f\"No path found\")\n"
   ],
   "id": "268b28f23ef0c550",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T15:13:54.185876Z",
     "start_time": "2025-09-30T15:13:53.953755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RIGHT = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "UP = 3\n",
    "\n",
    "RIGHT_DOWN = 4\n",
    "LEFT_DOWN = 5\n",
    "RIGHT_UP = 6\n",
    "LEFT_UP = 7\n",
    "\n",
    "observation, reward, terminated, truncated, info = grid_mars_env.step(LEFT, verbose=True)\n",
    "#observation;"
   ],
   "id": "10cd1d729d4d591a",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test random per capire cose",
   "id": "1e521476db3e97e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T16:21:04.528730Z",
     "start_time": "2025-09-30T16:21:04.153700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess_observation(observation, device=\"cpu\", eps=1e-4, return_type=\"torch\"):\n",
    "    padding_number = 0\n",
    "    agent_position = observation[\"agent\"]\n",
    "    target_position = observation[\"target\"]\n",
    "\n",
    "    local_map = observation[\"local_map\"]\n",
    "    local_map_mask = observation[\"local_map_mask\"]\n",
    "    visited_locations = observation[\"visited_locations\"]\n",
    "    map_shape = local_map.shape\n",
    "\n",
    "    # Channel 0: altitude with mask\n",
    "    channel_zero = np.array([\n",
    "        [local_map[y, x] if local_map_mask[y, x] else np.nan\n",
    "         for x in range(map_shape[1])]\n",
    "        for y in range(map_shape[0])\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Channel 1: visited locations\n",
    "    channel_one = visited_locations.astype(np.float32)\n",
    "\n",
    "    # Channel 2: agent position\n",
    "    channel_two = np.full(map_shape, fill_value=padding_number, dtype=np.float32)\n",
    "    channel_two[agent_position[0], agent_position[1]] = 1.0\n",
    "    \n",
    "    # Channel 3: target position\n",
    "    channel_three = np.full(map_shape, fill_value=padding_number, dtype=np.float32)\n",
    "    channel_three[target_position[0], target_position[1]] = 1.0\n",
    "\n",
    "    # Stack channels\n",
    "    x = np.stack([channel_zero, channel_one, channel_two, channel_three], axis=0)  # (C, H, W)\n",
    "    \n",
    "    \n",
    "    # Normalize channel 0 (altitude)\n",
    "    altitude = x[0]\n",
    "    mask = np.isnan(altitude)\n",
    "    min_val = np.nanmin(altitude)\n",
    "    max_val = np.nanmax(altitude)\n",
    "    if min_val == max_val:\n",
    "        x[0] = np.zeros_like(altitude)\n",
    "    else:\n",
    "        x[0] = ((altitude - min_val) / (max_val - min_val)) + eps\n",
    "    x[0][mask] = padding_number  # sentinel for NaN\n",
    "\n",
    "    # Normalize channel 1 (visited locations)\n",
    "    x[1] = np.log1p(x[1])\n",
    "    mask = channel_one == 0.\n",
    "    x[1][mask] = padding_number\n",
    "    x[1][~mask] += eps \n",
    "\n",
    "    # Convert to torch tensor and add batch dimension\n",
    "    return torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "def _process_states_v2(states, n_frames_per_input=2, device=\"cpu\", step=2):\n",
    "    states = states.squeeze()\n",
    "    batch_size = states.shape[0]\n",
    "    \n",
    "    grouped_states_list = []\n",
    "    for i in range(0, batch_size, step):\n",
    "        if i + n_frames_per_input > batch_size:\n",
    "            break\n",
    "            \n",
    "        states_to_group = states[i : i + n_frames_per_input]\n",
    "        num_frames, num_dims, h, w = states_to_group.shape\n",
    "        reshaped_state = states_to_group.reshape(1, num_frames * num_dims, h, w)\n",
    "        \n",
    "        grouped_states_list.append(reshaped_state)\n",
    "    final_tensor = torch.cat(grouped_states_list, dim=0)\n",
    "    \n",
    "    return final_tensor.to(device)\n",
    "\n",
    "# [2, 4, 15, 15] -> [1, 8, 15, 15]\n",
    "\n",
    "def process_states(states, n_frames_per_input=2, device=\"cpu\", step=1):\n",
    "    states = states.squeeze()\n",
    "    # shape -> [batch_size, n_dimensions, map_size, map_size]\n",
    "    n_dimensions = states.shape[1]\n",
    "    map_size = states.shape[-1]\n",
    "    grouped_states = []\n",
    "\n",
    "    for i in range(0, 64, step):\n",
    "        if i + n_frames_per_input > 64:\n",
    "            break\n",
    "            \n",
    "        # shape -> [n_frames_per_input, n_dimensions, map_size, map_size]\n",
    "        states_to_group = states[i:i + n_frames_per_input]\n",
    "\n",
    "        # shape -> [1, n_dimensions*n_frames_per_input, map_size, map_size]\n",
    "        reshaped_states = states_to_group.reshape(1, n_dimensions*n_frames_per_input, map_size, map_size)\n",
    "        grouped_states.append(reshaped_states)\n",
    "\n",
    "    np_grouped_states = np.array(grouped_states)\n",
    "    return torch.tensor(np_grouped_states, dtype=torch.float32).to(device)\n",
    "\n",
    "obs_list = []\n",
    "\n",
    "for _ in range(64):\n",
    "    ok = preprocess_observation(obs)\n",
    "    obs_list.append(ok)\n",
    "\n",
    "obs_tensor = torch.stack(obs_list)\n",
    "_process_states_v2(obs_tensor).shape"
   ],
   "id": "70d80c27a4219761",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:34:59.904675Z",
     "start_time": "2025-09-30T09:34:59.612888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from policy_network import PolicyNetwork\n",
    "\n",
    "config = {\n",
    "           \"input_channels\": 3,\n",
    "           \"backbone\": [\n",
    "               {\"type\": \"conv\", \"out_channels\": 32, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1, \"activation\": \"relu\"},\n",
    "               {\"type\": \"pool\", \"mode\": \"max\", \"kernel_size\": 2},\n",
    "               {\"type\": \"conv\", \"out_channels\": 64, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1, \"activation\": \"relu\"},\n",
    "               {\"type\": \"pool\", \"mode\": \"max\", \"kernel_size\": 2}\n",
    "           ],\n",
    "           \"head_action\": [\n",
    "               {\"type\": \"fc\", \"out_features\": 128, \"activation\": \"relu\"},\n",
    "               {\"type\": \"fc\", \"out_features\": 8}\n",
    "           ],\n",
    "           \"head_value\": [\n",
    "               {\"type\": \"fc\", \"out_features\": 64, \"activation\": \"relu\"},\n",
    "               {\"type\": \"fc\", \"out_features\": 1}\n",
    "           ]\n",
    "       }\n",
    "\n",
    "policy_network = PolicyNetwork(config)\n",
    "\n",
    "policy_network.to(\"cuda\")\n",
    "action_probs, value = policy_network(ok)\n",
    "\n",
    "print(action_probs)\n",
    "print(value)"
   ],
   "id": "dd9b2fb0ed80248d",
   "execution_count": 9,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
